High-performance computing (HPC):
With the advancement in the technology, the data being generated is humungous, and the need for processing and segregating the data is increasing exponentially. The challenging part of processing the data is doing it in time. This demands for more processing power, which has been previously tackled with increasing processor frequencies (clock-rates). But, a limit has been reached by the semiconductor industry in increasing the clock-rates, beyond which, quantum effects which introduce unpredictability. Due to the limits in processing rate, we have moved on to multiple processing units to process the data in parallel. This processing of large data using many computing units is coined as High-performance computing. This HPC includes designing the algorithms, methodologies, and implementations to process data.
    In a single line, HPC is the use of parallel processing for running advanced application programs efficiently, reliably and quickly.Any algorithm which is implemented using HPC methodologies should be robust to the platform and hardware-scalable.In this work, we only focus on computer vision algorithms to implement using HPC.

HPC practices:
As described above, HPC is implementing compute intensive and bandwidth intensive applications on a hardware efficiently.There are several ways to achieve; It primarily depends on the algorithm we are working with, the algorithm should be analysed properly for the amount of data required, data dependency in the algorithm, its computational complexity, and type of the algorithm.Some of the algorithms are designed to work on desktop PC's, and Some are designed specially to work on small moving systems(Autonomous Cars and robots).Based on the algorithm desired HPC practices will be adopted, for IVI (In-Vehicle Infotainment) applications there is no need to have a great hardware like Desktop machine.
    Some practices need to add hardware to the master hardware and some need to change the algorithm according to the hardware provided.The first scenario is having a co-processor environment and the second scenario is dealing with the specially designed hardware for specific applications.In this work, both practices mentioned are used.

Conventional Algorithm Implementation bottlenecks:
A complex algorithm needs complex computations and more data to process.So there exist two types of bottlenecks,memory bottleneck and compute bottleneck.

Memory bottleneck:
The computations depend on the number of computing resources available but for fast processing data should be made available to all the compute resources with low latency.Since if the cache is not organized properly then data will not be available for some compute elements in cache so those elements needs to fetch the data from the main memory,fetching of which is limited by the memory bandwidth of the BUS ,this is also called memory wall.In computer vision applications there is a need to process on more data, accessing of which is limited by the bandwith of the memory BUS.

Compute bottleneck:
For complex computer vision algorithms there is a more need for compute resources.So in such cases, there is more demand for computing resources.If the computations need more complex operations which are beyond the capability of the computing resources , then the data need to wait until the previous operations to comple,this causes compute bottleneck.which is called compute bound.

Avoiding the bottlenecks:

By improving the locality of memory reference in the application data access, the cache performance  can be improved and main memory access will be avoided.By maintaing different cache's in the architecture for different purposes can avoid the memory wall.This can be compared with the NVIDIA CUDA(Compute Unified Device Architecture) architecture where different cache organizations are included.And also for  compute bound cases more processing elements  are made available.This is explained clearly in the following chapters.

