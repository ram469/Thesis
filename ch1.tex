High-performance computing (HPC):
With the advancement in the technology, the data being generated is humungous, and the need for processing and segregating the data is increasing exponentially. The challenging part of processing the data is doing it in time. This demands for more processing power, which has been previously tackled with increasing processor frequencies (clock-rates). But, a limit has been reached by the semiconductor industry in increasing the clock-rates, beyond which, quantum effects which introduce unpredictability. Due to the limits in processing rate, we have moved on to multiple processing units to process the data in parallel. This processing of large data using many computing units is coined as High-performance computing. This HPC includes designing the algorithms, methodologies, and implementations to process data.
    In a single line, HPC is the use of parallel processing for running advanced application programs efficiently, reliably and quickly.Any algorithm which is implemented using HPC methodologies should be robust to the platform and hardware-scalable.In this work, we only focus on computer vision algorithms to implement using HPC.

HPC practices:
As described above, HPC is implementing compute intensive and bandwidth intensive applications on a hardware efficiently.There are several ways to achieve; It primarily depends on the algorithm we are working with, the algorithm should be analysed properly for the amount of data required, data dependency in the algorithm, its computational complexity, and type of the algorithm.Some of the algorithms are designed to work on desktop PC's, and Some are designed specially to work on small moving systems(Autonomous Cars and robots).Based on the algorithm desired HPC practices will be adopted, for IVI (In-Vehicle Infotainment) applications there is no need to have Desktop machine like hardware.
    Some practices need to add hardware to the master hardware and some need to change the algorithm according to the hardware provided.The first scenario is having a co-processor environment and the second scenario is dealing with the specially designed hardware for specific applications.In this work, both practices mentioned are used.

Conventional Algorithm Implementation bottlenecks:
A complex algorithm needs complex computations and more data to process.The computations depend on the number of computing resources available but for fast processing data should be made available to all the compute resources with low latency.So compute and memory bottlenecks (or memory wall) will occur in a typical algorithm implementation.In computer vision algorithms there are more data to process since algorithms works on images, also there exist more complex operations like filtering, applying FFT, etc.So in such cases, there is more demand for computing resources and as well need for providing the data to the compute resources.

The data movement between the memory and the CPU will be slow if there exist a memory bottleneck.Even though the processor is capable of running at good speeds, but it is limited by memory bottleneck.If the computations need more complex operations which are beyond the capability of the computing resources available, then the data need to wait until the previous operations to comple.

Avoiding the compute and memory  bottlenecks:

By improving the locality of memory reference in the application data access,this improves the cache performance and minimizes the main memory access.By maintaing different cache's in the architecture for different purposes will remove the memory bottleneck.This can be compared with the NVIDIA CUDA(Compute Unified Device Architecture) architecture where different cache organizations are included.And also for computing more processing elements  are available.
